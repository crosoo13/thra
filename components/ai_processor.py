import json
import google.generativeai as genai
from . import config
from . import database_manager as db

try:
    genai.configure(api_key=config.GEMINI_API_KEY)
    gemini_flash_model = genai.GenerativeModel(config.GEMINI_FLASH_MODEL_NAME)
    gemini_pro_model = genai.GenerativeModel(config.GEMINI_PRO_MODEL_NAME)
    print("‚úÖ –£—Å–ø–µ—à–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π Gemini Flash –∏ Pro.")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Gemini AI: {e}")
    exit()

async def get_routing_decisions(messages):
    """
    –≠–¢–ê–ü 1: –í—ã–∑—ã–≤–∞–µ—Ç Gemini Flash –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è: –æ—Ç–≤–µ—Ç–∏—Ç—å –∏–ª–∏ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å.
    """
    print("   ü§ñ –≠—Ç–∞–ø 1: –û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–∞ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫—É –≤ Gemini Flash...")
    prompt_template = db.get_prompt_template("router_prompt")
    if not prompt_template:
        print("   ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω 'router_prompt' –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö!")
        return []

    messages_for_prompt = [
        {"message_id": msg.id, "text": msg.text.strip()}
        for msg in messages if msg.text and not msg.text.isspace()
    ]

    if not messages_for_prompt:
        return []

    messages_json = json.dumps(messages_for_prompt, ensure_ascii=False, indent=4)
    full_prompt = prompt_template.replace('{messages_for_prompt}', messages_json)

    try:
        response = await gemini_flash_model.generate_content_async(full_prompt)
        cleaned_response_text = response.text.strip().removeprefix('```json').removesuffix('```')
        decisions = json.loads(cleaned_response_text)
        print(f"   ‚úÖ –°–æ—Ä—Ç–∏—Ä–æ–≤—â–∏–∫ –ø—Ä–∏–Ω—è–ª {len(decisions)} —Ä–µ—à–µ–Ω–∏–π.")
        return decisions
    except Exception as e:
        print(f"   ‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏: {e}")
        return []

async def generate_final_reply(message, persona: str, chat_id: int):
    """
    –≠–¢–ê–ü 2: –í—ã–∑—ã–≤–∞–µ—Ç Gemini Pro –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –ª–∏—á–Ω–æ—Å—Ç–∏.
    """
    # üëáüëáüëá –í–û–¢ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï! üëáüëáüëá
    # –ü—Ä–∏–≤–æ–¥–∏–º –∏–º—è –ø–µ—Ä—Å–æ–Ω—ã –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –ø–µ—Ä–µ–¥ –ø–æ–∏—Å–∫–æ–º –≤ –ë–î
    prompt_name = f"{persona.lower()}_prompt"
    
    print(f"   ü§ñ –≠—Ç–∞–ø 2: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ª–∏—á–Ω–æ—Å—Ç—å—é '{persona}' (–∑–∞–ø—Ä–æ—Å –ø—Ä–æ–º–ø—Ç–∞: '{prompt_name}')...")

    prompt_template = db.get_prompt_template(prompt_name)
    if not prompt_template:
        print(f"   ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω –ø—Ä–æ–º–ø—Ç '{prompt_name}' –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö!")
        return None

    full_prompt = prompt_template.replace('{message_text}', message.text)

    try:
        response = await gemini_pro_model.generate_content_async(full_prompt)
        print("\n--- üì• –û—Ç–≤–µ—Ç –æ—Ç Gemini Pro ---\n", response.text, "\n--------------------------\n")

        cleaned_response_text = response.text.strip().removeprefix('```json').removesuffix('```')
        ai_actions = json.loads(cleaned_response_text)

        if not ai_actions:
            print(f"   ‚ÑπÔ∏è –ü–µ—Ä—Å–æ–Ω–∞ '{persona}' —Ä–µ—à–∏–ª–∞ –Ω–µ –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ.")
            return None

        action = ai_actions[0]

        action.update({
            'target_chat_id': chat_id,
            'action_type': 'reply',
            'model_version': config.GEMINI_PRO_MODEL_NAME,
            'prompt_version': prompt_name,
            'original_message_text': message.text
        })
        return action

    except Exception as e:
        print(f"   ‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
        return None