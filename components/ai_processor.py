# components/ai_processor.py

import json
import google.generativeai as genai
from . import config
from . import database_manager as db

try:
    genai.configure(api_key=config.GEMINI_API_KEY)
    gemini_flash_model = genai.GenerativeModel(config.GEMINI_FLASH_MODEL_NAME)
    gemini_pro_model = genai.GenerativeModel(config.GEMINI_PRO_MODEL_NAME)
    print("‚úÖ –£—Å–ø–µ—à–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π Gemini Flash –∏ Pro.")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Gemini AI: {e}")
    exit()

# --- –§–£–ù–ö–¶–ò–û–ù–ê–õ "–ê–ì–ï–ù–¢ –í–õ–ò–Ø–ù–ò–Ø" ---

async def get_routing_decisions(messages):
    """
    –≠–¢–ê–ü 1: –í—ã–∑—ã–≤–∞–µ—Ç Gemini Flash –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è: –æ—Ç–≤–µ—Ç–∏—Ç—å –∏–ª–∏ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å.
    """
    print("  ü§ñ –≠—Ç–∞–ø 1 (–ê–≥–µ–Ω—Ç –≤–ª–∏—è–Ω–∏—è): –û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–∞ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫—É...")
    prompt_template = db.get_prompt_template("router_prompt")
    if not prompt_template:
        print("     ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω 'router_prompt' –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö!")
        return []

    messages_for_prompt = [
        {"message_id": msg.id, "text": msg.text.strip()}
        for msg in messages if msg.text and not msg.text.isspace()
    ]

    if not messages_for_prompt:
        return []

    messages_json = json.dumps(messages_for_prompt, ensure_ascii=False, indent=4)
    full_prompt = prompt_template.replace('{messages_for_prompt}', messages_json)

    try:
        # --- –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –ü–û–î–°–ß–ï–¢ –¢–û–ö–ï–ù–û–í ---
        try:
            token_count = await gemini_flash_model.count_tokens_async(full_prompt)
            print(f"    üìä –î–ª–∏–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –°–æ—Ä—Ç–∏—Ä–æ–≤—â–∏–∫–∞: {token_count.total_tokens} —Ç–æ–∫–µ–Ω–æ–≤.")
        except Exception as e:
            print(f"    ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥—Å—á–∏—Ç–∞—Ç—å —Ç–æ–∫–µ–Ω—ã –¥–ª—è –°–æ—Ä—Ç–∏—Ä–æ–≤—â–∏–∫–∞: {e}")
        # --- –ö–û–ù–ï–¶ –ù–û–í–û–ô –õ–û–ì–ò–ö–ò ---

        response = await gemini_flash_model.generate_content_async(full_prompt)
        
        # --- –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –ü–†–û–í–ï–†–ö–ê –û–¢–í–ï–¢–ê ---
        if not response.parts:
            print(f"     ‚ùå –û—à–∏–±–∫–∞ (–°–æ—Ä—Ç–∏—Ä–æ–≤—â–∏–∫): Gemini Flash –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç. –ü—Ä–∏—á–∏–Ω–∞: {response.candidates[0].finish_reason.name}")
            return []
        # --- –ö–û–ù–ï–¶ –ù–û–í–û–ô –õ–û–ì–ò–ö–ò ---

        cleaned_response_text = response.text.strip().removeprefix('```json').removesuffix('```')
        decisions = json.loads(cleaned_response_text)
        print(f"     ‚úÖ –°–æ—Ä—Ç–∏—Ä–æ–≤—â–∏–∫ (–ê–≥–µ–Ω—Ç –≤–ª–∏—è–Ω–∏—è) –ø—Ä–∏–Ω—è–ª {len(decisions)} —Ä–µ—à–µ–Ω–∏–π.")
        return decisions
    except Exception as e:
        print(f"     ‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ (–ê–≥–µ–Ω—Ç –≤–ª–∏—è–Ω–∏—è): {e}")
        return []


async def generate_final_reply(conversation_history, persona: str, chat_id: int, my_id: int):
    """
    –≠–¢–ê–ü 2: –í—ã–∑—ã–≤–∞–µ—Ç Gemini Pro –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–ø–∏—Å–∫–∏.
    """
    prompt_name = f"{persona.lower()}_prompt"
    print(f"  ü§ñ –≠—Ç–∞–ø 2 (–ê–≥–µ–Ω—Ç –≤–ª–∏—è–Ω–∏—è): –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ª–∏—á–Ω–æ—Å—Ç—å—é '{persona}'...")

    prompt_template = db.get_prompt_template(prompt_name)
    if not prompt_template:
        print(f"     ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω –ø—Ä–æ–º–ø—Ç '{prompt_name}' –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö!")
        return None

    # --- –§–û–†–ú–ò–†–û–í–ê–ù–ò–ï –ü–†–ò–ú–ï–†–û–í ---
    good_examples = db.get_examples_by_status(prompt_name, status='approved', limit=10)
    good_examples_string = ""
    if good_examples:
        example_texts = [f"–ò—Å—Ö–æ–¥–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: {ex['original_message_text']}\n–¢–≤–æ–π —É–¥–∞—á–Ω—ã–π –æ—Ç–≤–µ—Ç: {ex['ai_generated_text']}" for ex in good_examples if ex.get('original_message_text') and ex.get('ai_generated_text')]
        if example_texts:
            good_examples_string = "–í–æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–≤–µ–∂–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Ç–≤–æ–∏—Ö —É–¥–∞—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –≤ —ç—Ç–æ–π —Ä–æ–ª–∏. –ò–∑—É—á–∏ –∏—Ö, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–≤–æ–π —Å—Ç–∏–ª—å:\n" + "\n---\n".join(example_texts)

    bad_examples = db.get_examples_by_status(prompt_name, status='declined', limit=10)
    bad_examples_string = ""
    if bad_examples:
        example_texts = [f"–ò—Å—Ö–æ–¥–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: {ex['original_message_text']}\n–¢–≤–æ–π –ù–ï–£–î–ê–ß–ù–´–ô –æ—Ç–≤–µ—Ç: {ex['ai_generated_text']}" for ex in bad_examples if ex.get('original_message_text') and ex.get('ai_generated_text')]
        if example_texts:
            bad_examples_string = "\n–ê –≤–æ—Ç —Ç–∞–∫ –¥–µ–ª–∞—Ç—å –ù–ï –ù–ê–î–û. –≠—Ç–æ –ø—Ä–∏–º–µ—Ä—ã —Ç–≤–æ–∏—Ö –Ω–µ–¥–∞–≤–Ω–∏—Ö –Ω–µ—É–¥–∞—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω—ã. –ò–∑—É—á–∏ –∏—Ö, —á—Ç–æ–±—ã –Ω–µ –ø–æ–≤—Ç–æ—Ä—è—Ç—å –æ—à–∏–±–æ–∫:\n" + "\n---\n".join(example_texts)

    # --- –°–ë–û–†–ö–ê –§–ò–ù–ê–õ–¨–ù–û–ì–û –ü–†–û–ú–¢–ê ---
    prompt_with_good = prompt_template.replace('{dynamic_examples}', good_examples_string)
    prompt_with_bad = prompt_with_good.replace('{bad_examples}', bad_examples_string)

    history_for_prompt = []
    for msg in conversation_history:
        history_for_prompt.append({
            "id": msg.id,
            "author": "me" if msg.sender_id == my_id else "user",
            "text": msg.text.strip() if msg.text else ""
        })
    
    history_json_string = json.dumps(history_for_prompt, ensure_ascii=False, indent=4)
    full_prompt = prompt_with_bad.replace('{conversation_history_json}', history_json_string)
    
    try:
        # --- –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –ü–û–î–°–ß–ï–¢ –¢–û–ö–ï–ù–û–í ---
        try:
            token_count = await gemini_pro_model.count_tokens_async(full_prompt)
            print(f"    üìä –î–ª–∏–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ '{persona}': {token_count.total_tokens} —Ç–æ–∫–µ–Ω–æ–≤.")
        except Exception as e:
            print(f"    ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥—Å—á–∏—Ç–∞—Ç—å —Ç–æ–∫–µ–Ω—ã –¥–ª—è –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞: {e}")
        # --- –ö–û–ù–ï–¶ –ù–û–í–û–ô –õ–û–ì–ò–ö–ò ---

        response = await gemini_pro_model.generate_content_async(full_prompt)
        
        # --- –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –ü–†–û–í–ï–†–ö–ê –û–¢–í–ï–¢–ê, –ß–¢–û–ë–´ –°–ö–†–ò–ü–¢ –ù–ï –ü–ê–î–ê–õ ---
        if not response.parts:
            print(f"     ‚ùå –û—à–∏–±–∫–∞ (–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä): Gemini Pro –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç. –ü—Ä–∏—á–∏–Ω–∞: {response.candidates[0].finish_reason.name}")
            return None # –ü—Ä–æ—Å—Ç–æ –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Ñ—É–Ω–∫—Ü–∏–∏, –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç
        # --- –ö–û–ù–ï–¶ –ù–û–í–û–ô –õ–û–ì–ò–ö–ò ---
        
        cleaned_response_text = response.text.strip().removeprefix('```json').removesuffix('```')
        ai_actions = json.loads(cleaned_response_text)

        if not ai_actions:
            return None

        action = ai_actions[0]
        
        original_message = conversation_history[-1] if conversation_history else None
        original_message_text = original_message.text if original_message else ""
        target_user_id = original_message.sender_id if original_message else None

        action.update({
            'target_chat_id': chat_id,
            'target_user_id': target_user_id,
            'action_type': 'reply',
            'model_version': config.GEMINI_PRO_MODEL_NAME,
            'prompt_version': prompt_name,
            'original_message_text': original_message_text,
            'persona': persona
        })
        return action

    except json.JSONDecodeError:
        print(f"     ‚ùå –û—à–∏–±–∫–∞ (–ê–≥–µ–Ω—Ç –≤–ª–∏—è–Ω–∏—è): Gemini Pro –≤–µ—Ä–Ω—É–ª –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON.")
        return None
    except Exception as e:
        print(f"     ‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ (–ê–≥–µ–Ω—Ç –≤–ª–∏—è–Ω–∏—è): {e}")
        return None


# --- –§–£–ù–ö–¶–ò–û–ù–ê–õ "–û–•–û–¢–ù–ò–ö –ó–ê –õ–ò–î–ê–ú–ò" ---

async def get_lead_decisions(messages):
    """
    –≠–¢–ê–ü 1 (–û—Ö–æ—Ç–Ω–∏–∫): –í—ã–∑—ã–≤–∞–µ—Ç Gemini Flash –¥–ª—è –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –ª–∏–¥–æ–≤.
    """
    print("  üïµÔ∏è‚Äç‚ôÇÔ∏è –≠—Ç–∞–ø 1 (–û—Ö–æ—Ç–Ω–∏–∫): –û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –ª–∏–¥–æ–≤...")
    prompt_template = db.get_prompt_template("lead_finder_prompt")
    if not prompt_template:
        print("     ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω 'lead_finder_prompt' –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö!")
        return []

    messages_for_prompt = [
        {"message_id": msg.id, "text": msg.text.strip()}
        for msg in messages if msg.text and not msg.text.isspace()
    ]

    if not messages_for_prompt:
        return []

    messages_json = json.dumps(messages_for_prompt, ensure_ascii=False, indent=4)
    full_prompt = prompt_template.replace('{messages_for_prompt}', messages_json)

    try:
        # --- –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –ü–û–î–°–ß–ï–¢ –¢–û–ö–ï–ù–û–í ---
        try:
            token_count = await gemini_flash_model.count_tokens_async(full_prompt)
            print(f"    üìä –î–ª–∏–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –ª–∏–¥–æ–≤: {token_count.total_tokens} —Ç–æ–∫–µ–Ω–æ–≤.")
        except Exception as e:
            print(f"    ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥—Å—á–∏—Ç–∞—Ç—å —Ç–æ–∫–µ–Ω—ã –¥–ª—è –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –ª–∏–¥–æ–≤: {e}")
        # --- –ö–û–ù–ï–¶ –ù–û–í–û–ô –õ–û–ì–ò–ö–ò ---

        response = await gemini_flash_model.generate_content_async(full_prompt)

        # --- –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –ü–†–û–í–ï–†–ö–ê –û–¢–í–ï–¢–ê ---
        if not response.parts:
            print(f"    ‚ùå –û—à–∏–±–∫–∞ (–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ª–∏–¥–æ–≤): Gemini Flash –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç. –ü—Ä–∏—á–∏–Ω–∞: {response.candidates[0].finish_reason.name}")
            return []
        # --- –ö–û–ù–ï–¶ –ù–û–í–û–ô –õ–û–ì–ò–ö–ò ---

        cleaned_response_text = response.text.strip().removeprefix('```json').removesuffix('```')
        decisions = json.loads(cleaned_response_text)
        print(f"     ‚úÖ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä (–û—Ö–æ—Ç–Ω–∏–∫) –ø—Ä–∏–Ω—è–ª {len(decisions)} —Ä–µ—à–µ–Ω–∏–π.")
        return decisions
    except Exception as e:
        print(f"     ‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ª–∏–¥–æ–≤ (–û—Ö–æ—Ç–Ω–∏–∫): {e}")
        return []


async def generate_lead_outreach_message(target_message):
    """
    –≠–¢–ê–ü 2 (–û—Ö–æ—Ç–Ω–∏–∫): –í—ã–∑—ã–≤–∞–µ—Ç Gemini Pro –¥–ª—è –ì–ï–ù–ï–†–ê–¶–ò–ò –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –ª–∏–¥—É.
    """
    print(f"  üïµÔ∏è‚Äç‚ôÇÔ∏è –≠—Ç–∞–ø 2 (–û—Ö–æ—Ç–Ω–∏–∫): –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –ª–∏–¥–∞...")
    prompt_template = db.get_prompt_template("lead_outreach_prompt")
    if not prompt_template:
        print("     ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω 'lead_outreach_prompt' –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö!")
        return None
    
    lead_message_info = {
        "–ò–º—è": target_message.sender.first_name,
        "–°–æ–æ–±—â–µ–Ω–∏–µ": target_message.text.strip()
    }
    lead_message_json = json.dumps(lead_message_info, ensure_ascii=False, indent=4)
    
    full_prompt = prompt_template.replace('{lead_message_json}', lead_message_json)

    try:
        # --- –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –ü–û–î–°–ß–ï–¢ –¢–û–ö–ï–ù–û–í ---
        try:
            token_count = await gemini_pro_model.count_tokens_async(full_prompt)
            print(f"    üìä –î–ª–∏–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –ª–∏–¥–æ–≤: {token_count.total_tokens} —Ç–æ–∫–µ–Ω–æ–≤.")
        except Exception as e:
            print(f"    ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥—Å—á–∏—Ç–∞—Ç—å —Ç–æ–∫–µ–Ω—ã –¥–ª—è –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –ª–∏–¥–æ–≤: {e}")
        # --- –ö–û–ù–ï–¶ –ù–û–í–û–ô –õ–û–ì–ò–ö–ò ---

        response = await gemini_pro_model.generate_content_async(full_prompt)

        # --- –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –ü–†–û–í–ï–†–ö–ê –û–¢–í–ï–¢–ê ---
        if not response.parts:
            print(f"    ‚ùå –û—à–∏–±–∫–∞ (–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ª–∏–¥–æ–≤): Gemini Pro –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç. –ü—Ä–∏—á–∏–Ω–∞: {response.candidates[0].finish_reason.name}")
            return None
        # --- –ö–û–ù–ï–¶ –ù–û–í–û–ô –õ–û–ì–ò–ö–ò ---

        cleaned_response_text = response.text.strip().removeprefix('```json').removesuffix('```')
        action = json.loads(cleaned_response_text)
        print(f"     ‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —Ç–µ–∫—Å—Ç –¥–ª—è –ª–∏–¥–∞: {target_message.sender.first_name}")
        return action

    except json.JSONDecodeError:
        print(f"     ‚ùå –û—à–∏–±–∫–∞ (–û—Ö–æ—Ç–Ω–∏–∫): Gemini Pro –≤–µ—Ä–Ω—É–ª –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON.")
        return None
    except Exception as e:
        print(f"     ‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏—è –ª–∏–¥—É (–û—Ö–æ—Ç–Ω–∏–∫): {e}")
        return None