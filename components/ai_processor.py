# ai_processor.py

import json
import google.generativeai as genai
from . import config
from . import database_manager as db

try:
    genai.configure(api_key=config.GEMINI_API_KEY)
    gemini_flash_model = genai.GenerativeModel(config.GEMINI_FLASH_MODEL_NAME)
    gemini_pro_model = genai.GenerativeModel(config.GEMINI_PRO_MODEL_NAME)
    print("‚úÖ –£—Å–ø–µ—à–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π Gemini Flash –∏ Pro.")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Gemini AI: {e}")
    exit()

async def get_routing_decisions(messages):
    """
    –≠–¢–ê–ü 1: –í—ã–∑—ã–≤–∞–µ—Ç Gemini Flash –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è: –æ—Ç–≤–µ—Ç–∏—Ç—å –∏–ª–∏ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å.
    """
    print("  ü§ñ –≠—Ç–∞–ø 1: –û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–∞ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫—É –≤ Gemini Flash...")
    prompt_template = db.get_prompt_template("router_prompt")
    if not prompt_template:
        print("    ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω 'router_prompt' –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö!")
        return []

    messages_for_prompt = [
        {"message_id": msg.id, "text": msg.text.strip()}
        for msg in messages if msg.text and not msg.text.isspace()
    ]

    if not messages_for_prompt:
        return []

    messages_json = json.dumps(messages_for_prompt, ensure_ascii=False, indent=4)
    full_prompt = prompt_template.replace('{messages_for_prompt}', messages_json)

    try:
        response = await gemini_flash_model.generate_content_async(full_prompt)
        cleaned_response_text = response.text.strip().removeprefix('```json').removesuffix('```')
        decisions = json.loads(cleaned_response_text)
        print(f"    ‚úÖ –°–æ—Ä—Ç–∏—Ä–æ–≤—â–∏–∫ –ø—Ä–∏–Ω—è–ª {len(decisions)} —Ä–µ—à–µ–Ω–∏–π.")
        return decisions
    except Exception as e:
        print(f"    ‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏: {e}")
        return []

async def generate_final_reply(message, persona: str, chat_id: int):
    """
    –≠–¢–ê–ü 2: –í—ã–∑—ã–≤–∞–µ—Ç Gemini Pro –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞,
    –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –ø–æ–¥–≥—Ä—É–∂–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.
    """
    prompt_name = f"{persona.lower()}_prompt"
    print(f"  ü§ñ –≠—Ç–∞–ø 2: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ª–∏—á–Ω–æ—Å—Ç—å—é '{persona}' (–ø—Ä–æ–º–ø—Ç: '{prompt_name}')...")

    prompt_template = db.get_prompt_template(prompt_name)
    if not prompt_template:
        print(f"    ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω –ø—Ä–æ–º–ø—Ç '{prompt_name}' –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö!")
        return None

    # 1. –ü–æ–ª—É—á–∞–µ–º —Å–≤–µ–∂–∏–µ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –ë–î –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–æ–º—Ç–∞
    examples = db.get_approved_examples(prompt_name, limit=10)
    
    # 2. –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä—ã –≤ –µ–¥–∏–Ω—É—é —Å—Ç—Ä–æ–∫—É
    if examples:
        example_texts = []
        for ex in examples:
            if ex.get('original_message_text') and ex.get('ai_generated_text'):
                example_texts.append(
                    f"–ò—Å—Ö–æ–¥–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: {ex['original_message_text']}\n–¢–≤–æ–π —É–¥–∞—á–Ω—ã–π –æ—Ç–≤–µ—Ç: {ex['ai_generated_text']}"
                )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø—Ä–∏–º–µ—Ä—ã –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –∏ –±—ã–ª–∏ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω—ã
        if example_texts:
            examples_heading = "–í–æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–≤–µ–∂–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Ç–≤–æ–∏—Ö —É–¥–∞—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –≤ —ç—Ç–æ–π —Ä–æ–ª–∏. –ò–∑—É—á–∏ –∏—Ö, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–≤–æ–π —Å—Ç–∏–ª—å:\n"
            examples_string = examples_heading + "\n---\n".join(example_texts)
        else:
            examples_string = ""
    else:
        # –ï—Å–ª–∏ –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–µ—Ç, –æ—Å—Ç–∞–≤–ª—è–µ–º –±–ª–æ–∫ –ø—É—Å—Ç—ã–º
        examples_string = ""
        
    # 3. –í—Å—Ç–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã –≤ –ø—Ä–æ–º–ø—Ç
    prompt_with_examples = prompt_template.replace('{dynamic_examples}', examples_string)

    # 4. –ì–æ—Ç–æ–≤–∏–º JSON —Å —Ç–µ–∫—É—â–∏–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
    message_data_for_prompt = {
        "id": message.id,
        "text": message.text.strip()
    }
    message_json_string = json.dumps(message_data_for_prompt, ensure_ascii=False)
    
    # 5. –í—Å—Ç–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –ø—Ä–æ–º–ø—Ç
    full_prompt = prompt_with_examples.replace('{message_json}', message_json_string)
    
    try:
        response = await gemini_pro_model.generate_content_async(full_prompt)
        print("\n--- üì• –û—Ç–≤–µ—Ç –æ—Ç Gemini Pro ---\n", response.text, "\n--------------------------\n")

        cleaned_response_text = response.text.strip().removeprefix('```json').removesuffix('```')
        ai_actions = json.loads(cleaned_response_text)

        if not ai_actions:
            print(f"    ‚ÑπÔ∏è –ü–µ—Ä—Å–æ–Ω–∞ '{persona}' —Ä–µ—à–∏–ª–∞ –Ω–µ –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ.")
            return None

        action = ai_actions[0]

        action.update({
            'target_chat_id': chat_id,
            'action_type': 'reply',
            'model_version': config.GEMINI_PRO_MODEL_NAME,
            'prompt_version': prompt_name,
            'original_message_text': message.text
        })
        return action

    except json.JSONDecodeError:
        print(f"    ‚ùå –û—à–∏–±–∫–∞: Gemini Pro –≤–µ—Ä–Ω—É–ª –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON. –û—Ç–≤–µ—Ç –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω –≤—ã—à–µ.")
        return None
    except Exception as e:
        print(f"    ‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
        return None